# -*- coding: utf-8 -*-
"""simple-baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VT6zk2mLjmNp58TLJNcvXLD-Mjw7fP5Q
"""

import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/drive')
df = pd.read_csv('/drive/My Drive/cleaned_news.csv')

# -*- coding: utf-8 -*-
"""530 Evaluation Script

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkmralQ8Iu8hTga6VNcKQsgxFanyCU63

# Evaluation Metrics: Accuracy, Precision, Recall, and F-score
"""

## Input: y_pred, a list of length n with the predicted labels,
## y_true, a list of length n with the true labels

## Calculates the accuracy of the predicted labels
def get_accuracy(y_pred, y_true):
    C = 0
    I = 0
    for i in range(len(y_pred)):
        pred = y_pred[i]
        true = y_true[i]
        if pred == true:
            C += 1
        else:
            I += 1
    if(C == 0):
        return 0
    accuracy = C/(C + I)
    return accuracy


## Calculates the precision of the predicted labels
def get_precision(y_pred, y_true):
    TP = 0
    FP = 0
    for i in range(len(y_pred)):
        pred = y_pred[i]
        true = y_true[i]
        if pred == 1 and true == 1:
            TP += 1
        if pred == 1 and true == 0:
            FP += 1
    if(TP == 0):
        return 0
    precision = TP/(FP + TP)
    return precision

## Calculates the recall of the predicted labels
def get_recall(y_pred, y_true):
    TP = 0
    FN = 0
    for i in range(len(y_pred)):
        pred = y_pred[i]
        true = y_true[i]
        if pred == 1 and true == 1:
            TP += 1
        if pred == 0 and true == 1:
            FN += 1
    if(TP == 0):
        return 0
    recall = TP/(FN + TP)
    return recall

## Calculates the f1 score of the predicted labels
def get_fscore(y_pred, y_true):
    recall = get_recall(y_pred, y_true)
    precision = get_precision(y_pred, y_true)
    fscore = 2*(recall * precision)/(recall + precision)
    return fscore

## Function to calculate and print all the evaluation metrics
def test_predictions(y_pred, y_true):
    accuracy = get_accuracy(y_pred, y_true)
    recall = get_recall(y_pred, y_true)
    precision = get_precision(y_pred, y_true)
    fscore = get_fscore(y_pred, y_true)
    print("Accuracy: " + str(accuracy) + "\n")
    print("Recall: " + str(recall) + "\n")
    print("Precision: " + str(precision) + "\n")
    print("Fscore: " + str(fscore) + "\n")

## Example Output
A = [1, 0, 1, 1, 1, 1, 0, 1, 0, 0]
B = [1, 1, 0, 1, 1, 0, 0, 1, 0, 0]
test_predictions(A, B)

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df['split_text'] = df['text'].str.split()
df['split_title'] = df['title'].str.split()
df['length_text'] = df['split_text'].apply(lambda x: len(x))
df['length_title'] = df['split_title'].apply(lambda x: len(x))

X = df[['length_title', 'length_text']].to_numpy()
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

clf = LogisticRegression(random_state=0).fit(X, y)

y_pred = clf.predict(X_test)
test_predictions(list(y_pred), list(y_test))